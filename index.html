<html> 

<head>
<title> Audrey Huang </title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Audrey Huang">
<meta name="keywords" content="Huang, Audrey, Reinforcement Learning, CMU">

<link rel="stylesheet" type="text/css" href="styles.css">
</head>

<body>
    <!-- <table border='0' width='100%' max-width='700px' > -->
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>
        <tr>
        <!-- <td width='60%' vertical-align='middle' padding='30px'> -->
        <td style='width:50%;vertical-align=middle;padding=10px;'>
            <p style='text-align:center'>
            <name> Audrey Huang </name>
            </p>
            <p style='text-align:center'>
                <a href="https://scholar.google.com/citations?hl=en&user=zl70inwAAAAJ&view_op=list_works&sortby=pubdate">google scholar</a> &nbsp|&nbsp
                <a href="mailto:audreynhuang@gmail.com"><font size='-0'>email</font></a> 
            </p>

            <p> 
                I am a research assistant affiliated with the <a href='http://acmilab.org/'>ACMI Lab</a> at Carnegie Mellon University, where I work on theoretical problems in reinforcement learning. 
            </p>
            <p>
                I am fortunate to be advised by <a href='http://zacklipton.com/'>Zachary Lipton</a> (CMU) and <a href='https://www.cs.purdue.edu/homes/kamyar/'>Kamyar Azizzadenesheli</a> (Purdue University), and work closely with <a href='https://www.cs.cmu.edu/~leqil/'>Leqi Liu</a>. 
                Previously, I obtained my master's degree from CMU's <a href='https://www.ml.cmu.edu/'> Machine Learning Department</a>, and did my undergrad at Caltech, where I worked primarily in biophysics. 
            </p>
            <p>
                I will begin my PhD in Fall 2021. Most weekends now I can be found at the <a href='https://www.nps.gov/neri/index.htm'>New River Gorge</a>.
            </p>
            

        </td>
        <!-- <td width='30%' vertical-align='middle' padding='500px'> -->
        <td style='width:30%;vertical-align=middle;padding=10px'>
            <img style='float:right' src='images/thumbnail.png' width='90%'>
        </td>
        </tr>
        </tbody>
    </table>

    <br>
    <br>
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>
        <tr>
        <td width='100%' vertical-align='middle' padding='5%'>
            <header>Research</header>
            <p> 
                I'm interested in understanding how reinforcement learning algorithms work (or don't work) from a theoretical perspective, with the aim of improving current methods using principled approaches. My recent work has focused on reinforcement learning and off-policy evaluation with risk metrics.
            </p>
        </td>
        </tr>
        </tbody>
    </table>

    <br>
    
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>

        <tr>
        <td vertical-align='middle'>
            <header>Selected Publications</header>
        </td>
        </tr>
        <br>

        <td>
            <tr >
            <td width='30%' vertical-align='middle' padding='30px'>
                <img src='images/opra.png' width='100%'>
            </td>
            <td width='80%' vertical-align='middle' padding='30px'>
                <a href='https://arxiv.org/abs/2104.08977'>
                <paper>Off Policy Risk Assessment in Contextual Bandits</paper>
                <br>
                </a>
                <strong>Audrey Huang</strong>,
                <a href='https://www.cs.cmu.edu/~leqil/'>Leqi Liu</a>,
                <a href='http://zacklipton.com/'>Zachary Lipton</a>, 
                <a href='https://www.cs.purdue.edu/homes/kamyar/'>Kamyar Azizzadenesheli</a>
                <br>
                <em>arXiv 2021</em>
                <br>
                <p> 
                    We estimate broad classes of popularly studied risk metrics from off-policy datasets by first estimating the CDF of returns. We derive finite sample error bounds for both importance sampling and doubly robust estimators.
                </p>
            </td>
            </tr>
        </td>

        <td>
            <br>
            <br>
            <tr >
            <td width='30%' vertical-align='middle' padding='10%'>
                <img src='images/mcr.png' width='95%'>
            </td>
            <td width='80%' vertical-align='middle' padding='10%'>
                <a href='https://arxiv.org/abs/2103.02827'>
                <paper>On the Convergence and Optimality of Policy Gradient for Markov Coherent Risk</paper>
                <br>
                </a>
                <strong>Audrey Huang</strong>,
                <a href='https://www.cs.cmu.edu/~leqil/'>Leqi Liu</a>,
                <a href='http://zacklipton.com/'>Zachary Lipton</a>, 
                <a href='https://www.cs.purdue.edu/homes/kamyar/'>Kamyar Azizzadenesheli</a>
                <br>
                <em>Under Review; NeurIPS 2020 Real World Reinforcement Learning Workshop</em>
                <br>
                <p> 
                    We investigate two open problems related to policy gradient for coherent risk, and (1) show that the objective is not gradient dominated, (2) propose a tractable method for estimating its gradient. 
                </p>
            </td>
            </tr>
        </td>

        <td>
            <br>
            <br>
            <br>
            <tr>
            <td width='20%' vertical-align='middle' padding='20px'>
                <img src='images/graph.png' width='95%'>
            </td>
            <td width='80%' vertical-align='middle' padding='20px'>
                <a href='https://arxiv.org/abs/1907.05518'>
                <paper>Graph-Structured Visual Imitation</paper>
                <br>
                </a>
                <a href='https://msieb1.github.io/'>Maximilian Sieb</a>,
                <a href='https://scholar.google.com/citations?user=Kbi2t9sAAAAJ&hl=en'>Zhou Xian</a>, 
                <strong>Audrey Huang</strong>,
                <a href='https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/'>Oliver Kroemer</a>,
                <a href='https://www.cs.cmu.edu/~katef/'>Katerina Fragkiadaki</a>
                <br>
                <em>CoRL 2019</em>
                <br>
                <p> 
                    Visual entity correspondence-based reward drives successful robotic imitation of manipulation tasks from videos. 
                </p>
            </td>
            </tr>
        </td>
        </tbody>
    </table>

    <td>
        <br>
        <br>
    </td>


</body>

</html>