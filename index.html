<html> 

<head>
<title> Audrey Huang </title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Audrey Huang">
<meta name="keywords" content="Huang, Audrey, Reinforcement Learning, UIUC, CMU">

<link rel="stylesheet" type="text/css" href="styles.css">
</head>

<body>
    <!-- <table border='0' width='100%' max-width='700px' > -->
    <table style='width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>
        <tr>
        <!-- <td width='60%' vertical-align='middle' padding='30px'> -->
        <td style='width:40%;vertical-align=middle;padding=10px;'>
            <br>
            <p style='text-align:center'>
            <name> Audrey Huang </name>
            </p>
            <p style="margin-top:30px;"> 
                I am a Computer Science PhD student at UIUC, where I am fortunate to be advised by <a href='https://nanjiang.cs.illinois.edu/'>Nan Jiang</a>. 
                <!-- Previously, I received my MS from CMU's Machine Learning Department and my BS from Caltech.  -->
                <bold>I work on reinforcement learning and, more broadly, interactive decision making.</bold> 
            </p>
            <p>    
                I'm thankful for incredible summers at Microsoft Research working with <a href='https://people.cs.umass.edu/~akshay/'>Akshay Krishnamurthy</a> and <a href='https://dylanfoster.net/'>Dylan Foster</a>; at Google Research with <a href='https://mohammadghavamzadeh.github.io/'>Mohammad Ghavamzadeh</a> and <a href='https://marek.petrik.us/'>Marek Petrik</a>; and at Adobe Research.  
                <!-- My research focuses on reinforcement learning.  -->
            </p>

            <p style='text-align:center;margin-top:-10px'>
                [<a href="https://scholar.google.com/citations?hl=en&user=zl70inwAAAAJ&view_op=list_works&sortby=pubdate">google scholar</a>] &nbsp&nbsp&nbsp
                [<a href="mailto:audreyh5@illinois.edu">email</a>] 
            </p>

        </td>
        <!-- <td width='30%' vertical-align='middle' padding='500px'> -->
        <td style='width:30%;vertical-align=middle;padding=10px'>
            <img src='images/thumbnail.jpg' width='70%' style="margin-left:30px; margin-top:30px">
        </td>
        </tr>
        </tbody>
    </table>

    <!-- <br> -->
    <br>
    <table style='width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>
        <tr>
        <!-- <td width='100%' max-width=600px vertical-align='left' padding='5%'> -->
        <td style='width:90%;vertical-align=top;padding=10px;'>
            <header style="margin-bottom:-2px">Research</header>
            <p> 
                <!-- My research focuses on reinforcement learning and, more broadly, interactive decision making.  -->
                I care about developing principled and implementable algorithms with provable guarantees.
                <bold>Current/previous threads include:</bold>  
            <ul style="margin-top:-11px;">
                <li>Online finetuning (e.g., of large language models)</li>
                <li>Imitation learning</li>
                <li>Tractable online exploration</li>
                <li>Offline RL and evaluation</li>
            </ul>
            </p>
            <p style="margin-top:-10px;">
                I believe that theoretical insights into fundamental questions will lead to real-world algorithmic improvements, and vice versa. 
            </p>
        </td>
        <td style='width:20%;vertical-align=middle;padding=10px'>
        </td>
        </tr>
        </tbody>
    </table>
    <br>
    
    <table style='width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>

        <tr>
        <td vertical-align='middle'>
            <header style="margin-bottom:15px">Selected Papers</header>
        </td>
        </tr>
        <!-- <br>  -->

        <td>
            <!-- <br> -->
            <a href='https://arxiv.org/pdf/2407.13399'>
            <paper>
                Correcting the Mythos of KL-Regularization: 
                Direct Alignment without Overoptimization via Chi-Squared
                Preference Optimization
            </paper>
            </a>
            <!-- <br> -->
            <br>
            <authors>
                (Preprint, 2024) Audrey Huang*, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay Krishnamurthy, Dylan J. Foster                
            </authors>
            <br>
            <descrip> 
                A one-line change to DPO derived from chi-squared regularization provably mitigates overoptimization. 
                <!-- Algorithms derived from KL-regularization (e.g., RLHF, DPO+SFT) have exponentially worse sample complexity.   -->
            </descrip>
        </br>
        </td>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://openreview.net/pdf?id=yJYVcItE3K'>
                <paper>Non-adaptive Online Finetuning for Offline Reinforcement Learning</paper>
                </a>
                <br>
                <authors>
                (RLC, 2024) Audrey Huang*, Mohammad Ghavamzadeh, Nan Jiang, Marek Petrik. 
                </authors>              
                <br>
                <descrip> 
                    
                    Given an offline dataset, how should online data be collected in order to maximize policy improvement? 
                    <!-- as linear functions of the density features (from the low-rank decomposition).  -->
                    <!-- prior works that focus on the left features, we leverage the right features, which induce models for state occupancies, for reward-free learning.  -->
                </descrip>
            </td>
        </tr>

        <!-- <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <paper>Timing as an Action: Learning when to Observe and Act</paper>
                <br>
                (under review, 2023) Helen Zhou*, <strong>Audrey Huang</strong>*, Zack Lipton, David Childers
                <br>
                <descrip> 
                    RL algos for a healthcare-motivated setting where the learner decides the # of timesteps each action is taken for, but doesn't observe the intermediate states.  
                </descrip>
            </td>
        </tr> -->

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2302.02252.pdf'>
                <paper>Reinforcement Learning in Low-Rank MDPs with Density Features</paper>
                </a>
                <br>
                <authors>
                (ICML 2023) Audrey Huang*, Jinglin Chen, Nan Jiang.  
                </authors>             
                <br>
                <descrip> 
                    Offline and online RL via the occupancy functions is sample-efficient in low-rank MDPs. A clean inductive error analysis tames error exponentiation. 
                    <!-- as linear functions of the density features (from the low-rank decomposition).  -->
                    <!-- prior works that focus on the left features, we leverage the right features, which induce models for state occupancies, for reward-free learning.  -->
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2210.15543.pdf'>
                <paper>Beyond the Return: Off-policy Function Estimation under User-specified<br> Error-measuring Distributions</paper>
                </a>
                <br>
                <author>
                (NeurIPS 2022) Audrey Huang*, Nan Jiang. 
                </author>              
                <br>
                <descrip> 
                    Regularization is key for accurate offline value and density-ratio estimation from general function approximators. 
                </descrip>
            </td>
        </tr>

        <!-- <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2206.13648.pdf'>
                <paper>Supervised Learning with General Risk Functionals </paper>
                </a>
                <br>
                (ICML 2022) Liu Leqi, <strong>Audrey Huang</strong>, Zachary Lipton, Kamyar Azizzadenesheli. 
                <br>
                <descrip> 
                    Uniform convergence and gradient-based optimization of general risk functionals using distribution-centric methods.   
                </descrip>
            </td>
        </tr> -->

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2202.04634.pdf'>
                <paper>Offline Reinforcement Learning with Realizability and Single-policy Concentrability
                </paper>
                </a>
                <br>
                <authors> 
                (COLT 2022) Wenhao Zhan, Baihe Huang, Audrey Huang*, Nan Jiang, Jason Lee. 
                </authors>
                <br>
                <descrip> 
                    With proper regularization, offline RL is sample-efficient given only realizable function classes, and data with single-policy coverage.    
                </descrip>
            </td>
        </tr>

        <!-- <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://proceedings.mlr.press/v151/huang22b/huang22b.pdf'>
                <paper>Off Policy Risk Assessment in Markov Decision Processes </paper>
                </a>
                <br>
                (AISTATS 2022) <strong>Audrey Huang</strong>, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli. 
                <br>
                <descrip> 
                    Extension of below paper to sequential MDPs using a novel CDF Bellman operator. Certain CDF estimators are better for certain plug-in risk estimates.   
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/abs/2104.08977'>
                <paper>Off Policy Risk Assessment in Contextual Bandits </paper>
                </a>
                <br>
                (NeurIPS 2021) <strong>Audrey Huang</strong>, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli. 
                <br>
                <descrip> 
                    Estimation of general risk measures from logged data using CDF + plug-in approach, with doubly robust CDF estimator for variance reduction.   
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/abs/1907.05518'>
                    <paper>Graph-Structured Visual Imitation</paper>
                    </a>
                    <br>
                    (CoRL 2019, spotlight) Maximilian Sieb, Zhou Xian, <strong>Audrey Huang</strong>, Oliver Kroemer, Katerina Fragkiadaki. 
                    <br>
                <descrip> 
                    Visual entity correspondence-based reward drives successful robotic imitation of manipulation tasks from videos. 
                </descrip>
            </td>
        </tr> -->

        </tbody>
    </table>

    <!-- <br> 
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>

        <tr>
        <td vertical-align='middle'>
            <header>Workshop Papers</header>
        </td>
        </tr>
        <br>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://openreview.net/pdf?id=qz9oT84Ufy9'>
                    <paper>RiskyZoo: A Library for Risk-Sensitive Supervised Learning</paper>
                    </a>
                    <br>
                    (ICML 2022, Workshop on Responsible Decision Making in Dynamic Environments)
                    <br>
                    William Wong, <strong>Audrey Huang</strong>, Liu Leqi, Kamyar Azizzadenesheli, Zachary Lipton.   
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/abs/2103.02827'>
                <paper>On the Convergence and Optimality of Policy Gradient for Markov Coherent Risk</paper>
                </a>
                <br>
                (NeurIPS 2020, Real World Reinforcement Learning Workshop)
                <br>
                <strong>Audrey Huang</strong>, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli. 
            </td>
        </tr>


        </tbody>
    </table> -->

    <td>
        <br>
        <br>
    </td>


</body>

</html>