<html> 

<head>
<title> Audrey Huang </title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Audrey Huang">
<meta name="keywords" content="Huang, Audrey, Reinforcement Learning, UIUC, CMU">

<link rel="stylesheet" type="text/css" href="styles.css">
</head>

<body>
    <!-- <table border='0' width='100%' max-width='700px' > -->
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>
        <tr>
        <!-- <td width='60%' vertical-align='middle' padding='30px'> -->
        <td style='width:40%;vertical-align=middle;padding=10px;'>
            <br>
            <p style='text-align:center'>
            <name> Audrey Huang </name>
            </p>
            <br>
            <p> 
                I am a Computer Science PhD student at UIUC, where I am fortunate to be advised by <a href='https://nanjiang.cs.illinois.edu/'>Nan Jiang</a>. 
                <!-- My research focuses on reinforcement learning.  -->
            </p>
            <p>
                Previously, I obtained my MS from CMU's Machine Learning Department, where I worked with <a href='http://zacklipton.com/'>Zack Lipton</a> and <a href='https://www.cs.purdue.edu/homes/kamyar/'>Kamyar Azizzadenesheli</a>, after completing a BS in Computer Science at Caltech.  
            </p>

            <p style='text-align:center'>
                <a href="https://scholar.google.com/citations?hl=en&user=zl70inwAAAAJ&view_op=list_works&sortby=pubdate">google scholar</a> &nbsp&nbsp&nbsp
                <a href="mailto:audreyh5@illinois.edu">email</a> 
            </p>

        </td>
        <!-- <td width='30%' vertical-align='middle' padding='500px'> -->
        <td style='width:30%;vertical-align=middle;padding=10px'>
            <img style='float:right' src='images/thumbnail.jpg' width='80%'>
        </td>
        </tr>
        </tbody>
    </table>

    <br>
    <br>
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>
        <tr>
        <!-- <td width='100%' max-width=600px vertical-align='left' padding='5%'> -->
        <td style='width:90%;vertical-align=middle;padding=10px;'>
            <header>Research</header>
            <p> 
                My research focuses on developing and analyzing reinforcement learning algorithms. 
                I care about <u>sample and computational efficiency</u>, and <u>alignment with human desiderata</u>. I commonly use tools from statistical learning theory, optimization, and behavioral economics. 
            </p>
        </td>
        <td style='width:20%;vertical-align=middle;padding=10px'>
        </td>
        </tr>
        </tbody>
    </table>

    <br>
    
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>

        <tr>
        <td vertical-align='middle'>
            <header>Publications</header>
        </td>
        </tr>
        <br> 

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://openreview.net/pdf?id=yJYVcItE3K'>
                <paper>Non-adaptive Online Finetuning for Offline Reinforcement Learning</paper>
                </a>
                <br>
                (under review, 2023) <strong>Audrey Huang</strong>, Mohammad Ghavamzadeh, Nan Jiang, Marek Petrik.               
                <br>
                <descrip> 
                    Given an offline dataset, how should we collect a non-adaptive online dataset in order to maximize improvement over the purely offline policy?
                    <!-- as linear functions of the density features (from the low-rank decomposition).  -->
                    <!-- prior works that focus on the left features, we leverage the right features, which induce models for state occupancies, for reward-free learning.  -->
                </descrip>
            </td>
        </tr>

        <!-- <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <paper>Timing as an Action: Learning when to Observe and Act</paper>
                <br>
                (under review, 2023) Helen Zhou*, <strong>Audrey Huang</strong>*, Zack Lipton, David Childers
                <br>
                <descrip> 
                    RL algorithms for a healthcare-motivated setting where the learner must choose how many timesteps to repeat each action for, but doesn't observe the intermediate states.  
                </descrip>
            </td>
        </tr> -->

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2302.02252.pdf'>
                <paper>Reinforcement Learning in Low-Rank MDPs with Density Features</paper>
                </a>
                <br>
                (ICML 2023) <strong>Audrey Huang</strong>*, Jinglin Chen*, Nan Jiang.               
                <br>
                <descrip> 
                    We model occupancies for reward-free RL in low-rank MDPs, with novel inductive error analysis to tame error exponentiation. 
                    <!-- as linear functions of the density features (from the low-rank decomposition).  -->
                    <!-- prior works that focus on the left features, we leverage the right features, which induce models for state occupancies, for reward-free learning.  -->
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2210.15543.pdf'>
                <paper>Beyond the Return: Off-policy Function Estimation under  User-specified<br> Error-measuring Distributions</paper>
                </a>
                <br>
                (NeurIPS 2022) <strong>Audrey Huang</strong>, Nan Jiang.               
                <br>
                <descrip> 
                    Regularization is key for accurate off-policy value and density-ratio estimation under general function approximation. 
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2206.13648.pdf'>
                <paper>Supervised Learning with General Risk Functionals </paper>
                </a>
                <br>
                (ICML 2022) Liu Leqi, <strong>Audrey Huang</strong>, Zachary Lipton, Kamyar Azizzadenesheli. 
                <br>
                <descrip> 
                    Uniform convergence and gradient-based optimization of general risk functionals using distribution-centric methods.   
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/pdf/2202.04634.pdf'>
                <paper>Offline Reinforcement Learning with Realizability and Single-policy Concentrability
                </paper>
                </a>
                <br>
                (COLT 2022) Wenhao Zhan, Baihe Huang, <strong>Audrey Huang</strong>, Nan Jiang, Jason Lee. 
                <br>
                <descrip> 
                    Sample-efficient offline learning under only realizability and single-policy concentratability, leveraging primal-dual MDP formulation and regularization.  
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://proceedings.mlr.press/v151/huang22b/huang22b.pdf'>
                <paper>Off Policy Risk Assessment in Markov Decision Processes </paper>
                </a>
                <br>
                (AISTATS 2022) <strong>Audrey Huang</strong>, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli. 
                <br>
                <descrip> 
                    Extension of below paper to sequential MDPs using a novel CDF Bellman operator. Certain CDF estimators are better for certain plug-in risk estimates.   
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/abs/2104.08977'>
                <paper>Off Policy Risk Assessment in Contextual Bandits </paper>
                </a>
                <br>
                (NeurIPS 2021) <strong>Audrey Huang</strong>, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli. 
                <br>
                <descrip> 
                    Estimation of general risk measures from logged data using CDF + plug-in approach, with doubly robust CDF estimator for variance reduction.   
                </descrip>
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/abs/1907.05518'>
                    <paper>Graph-Structured Visual Imitation</paper>
                    </a>
                    <br>
                    (CoRL 2019, spotlight) Maximilian Sieb, Zhou Xian, <strong>Audrey Huang</strong>, Oliver Kroemer, Katerina Fragkiadaki. 
                    <br>
                <descrip> 
                    Visual entity correspondence-based reward drives successful robotic imitation of manipulation tasks from videos. 
                </descrip>
            </td>
        </tr>

        </tbody>
    </table>

    <br> 
    <table style='width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;' >
        <tbody>

        <tr>
        <td vertical-align='middle'>
            <header>Workshop Papers</header>
        </td>
        </tr>
        <br>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://openreview.net/pdf?id=qz9oT84Ufy9'>
                    <paper>RiskyZoo: A Library for Risk-Sensitive Supervised Learning</paper>
                    </a>
                    <br>
                    (ICML 2022, Workshop on Responsible Decision Making in Dynamic Environments)
                    <br>
                    William Wong, <strong>Audrey Huang</strong>, Liu Leqi, Kamyar Azizzadenesheli, Zachary Lipton.   
            </td>
        </tr>

        <tr width='100%' vertical-align='middle' padding='30px'>
            <td>
                <br>
                <a href='https://arxiv.org/abs/2103.02827'>
                <paper>On the Convergence and Optimality of Policy Gradient for Markov Coherent Risk</paper>
                </a>
                <br>
                (NeurIPS 2020, Real World Reinforcement Learning Workshop)
                <br>
                <strong>Audrey Huang</strong>, Liu Leqi, Zachary Lipton, Kamyar Azizzadenesheli. 
            </td>
        </tr>


        </tbody>
    </table>

    <td>
        <br>
        <br>
    </td>


</body>

</html>